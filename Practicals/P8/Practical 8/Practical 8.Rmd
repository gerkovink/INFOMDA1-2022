
```{r}
library(tidyverse)
library(magrittr)
library(psych)
library(caret)
library(gbm)
library(xgboost)
library(data.table)
suppressWarnings(library(ggforce))
library(cowplot)
```

```{r }
set.seed(45)
df <- readRDS("data/train_disease.RDS")

```

```{r 1}
df %>% select(-c(Gender, Disease)) %>% 
  describe.by(df$Disease)
```

```{r 2}
df %>%
  select(-Gender) %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = Disease, fill = Disease)) +
  geom_boxplot(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  theme_minimal()

df %>%
  select(-Gender) %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = Disease, fill = Disease)) +
  geom_density(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  theme_minimal()

```

```{r 3}
#Bagging stands for Bootstrap Aggregating. It involves training multiple models independently on different random subsets of the data, and then averaging the predictions. Random Forest is a specific type of bagging algorithm that uses decision trees as the base model. Boosting is an ensemble technique that focuses on training models sequentially, where each model attempts to correct the mistakes of the previous model.

```

```{r 4}
cvcontrol <- trainControl(method = "repeatedcv", 
                          number = 10,
                          allowParallel = TRUE)

bag_train <- train(Disease ~ ., data = df, method = "treebag", trControl = cvcontrol, importance = TRUE)
```

```{r }
varImp(bag_train)

```

```{r 6}
pred_train <- predict(bag_train)

confusionMatrix(pred_train, df$Disease)

```

```{r 7}
bag_train

```

```{r 8}

ranf_train <- train(Disease ~ ., data = df, method = "rf", trControl = cvcontrol, importance = TRUE)

```

```{r 9}

varImp(ranf_train)
# According to the RF other variable are more important
```

```{r 10}

ranf_train
# Yes the accuracy is slightly better, but not by much
```

```{r 11}
boost_train <- train(Disease ~ ., data = df, method = "gbm", verbose = F, trControl = cvcontrol)

```

```{r 12}

summary(boost_train)
boost_train
#The accuracy for the best one seems better than the other 2 models.
```

```{r 14}
library(devtools)
source_url("https://github.com/pablo14/shap-values/blob/master/shap.R?raw=TRUE")

```
```{r }
train_x <- model.matrix(Disease ~ ., df)[,-1]
train_y <- as.numeric(df$Disease) - 1
xgboost_train <- xgboost(data = train_x,
                         label = train_y, 
                         max.depth = 10,
                         eta = 1,
                         nthread = 4,
                         nrounds = 4,
                         objective = "binary:logistic",
                         verbose = 2)



pred <- tibble(Disease = predict(xgboost_train, newdata = train_x)) %>%
  mutate(Disease = factor(ifelse(Disease < 0.5, 1, 2),
                          labels = c("Healthy", "Disease")))

table(pred$Disease, df$Disease)
```



```{r 16}
shap_results <- shap.score.rank(xgboost_train,
                                X_train = train_x,
                                shap_approx = F)

var_importance(shap_results)
```

```{r }
shap_long <- shap.prep(shap = shap_results,
                       X_train = train_x)

plot.shap.summary(shap_long)

xgb.plot.shap(train_x, features = colnames(train_x), model = xgboost_train, n_col = 3)
```

```{r }
test<- readRDS("data/test_disease.RDS")

test %>% mutate(bag_test = predict(bag_train, newdata = test)
rf_test  = predict(rf_train, newdata = test)
gbm_test = predict(gbm_train, newdata = test)
xgb_test = predict(xgboost_train, newdata = model.matrix(Disease ~ ., test)[,-1]) %>%
  factor(x = ifelse(. < 0.5, 1, 2), levels = c(1,2), labels = c("Healthy", "Disease")))

list(bag_test, 
     rf_test, 
     gbm_test, 
     xgb_test) %>%
  map(~ confusionMatrix(.x, test$Disease))
```

```{r }

```